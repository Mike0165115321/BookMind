"""
Entry Point ‚Äî Full RAG Pipeline (Query Transform ‚Üí Search ‚Üí Generate).

Supports two modes:
  - Classic:  [HyDE] ‚Üí Search ‚Üí Generate (single-shot)
  - Agentic:  Decompose ‚Üí Multi-hop Search ‚Üí Evaluate ‚Üí Generate (multi-hop)

Pipeline (Classic):
  1. [Optional] HyDE Query Transform (Groq LLaMA) ‚Äî improves search quality
  2. Hybrid Search (Dense + BM25) + Adaptive Reranking
  3. LLM Generation (Gemini 2.5 Flash) ‚Äî answers from retrieved context

Pipeline (Agentic):
  1. Query Decomposition ‚Äî break complex query into sub-queries
  2. Multi-hop Search ‚Äî search each sub-query, evaluate, loop if needed
  3. LLM Generation ‚Äî synthesize answer from all gathered chunks

Usage:
  python3 ask.py                         # Interactive mode (classic)
  python3 ask.py "your question"         # Single question (classic)
  python3 ask.py --agentic "question"    # Agentic mode (multi-hop)
  python3 ask.py --no-hyde "q"           # Disable HyDE
  python3 ask.py --no-stream "q"         # Disable streaming
"""
import sys
import time
import config
from rag_searcher import RAGSearcher
from core.llm_generator import generate
from core.query_transformer import hyde_transform
from core.agentic_controller import AgenticController


def ask(query: str, searcher: RAGSearcher, stream: bool = True, use_hyde: bool = True) -> str:
    """
    Classic RAG pipeline: [HyDE] ‚Üí Search ‚Üí Generate.

    Args:
        query: User's question
        searcher: Initialized RAGSearcher instance
        stream: Whether to stream the response
        use_hyde: Whether to use HyDE query transform

    Returns:
        Generated answer string
    """
    print(f"\n{'‚ïê' * 60}")
    print(f"‚ùì {query}")
    print(f"{'‚ïê' * 60}")

    # Stage 0: Query Transform (HyDE via Groq)
    search_query = query
    if use_hyde and config.ENABLE_HYDE:
        t_hyde = time.time()
        search_query = hyde_transform(query)
        hyde_time = time.time() - t_hyde
    else:
        hyde_time = 0

    # Stage 1+2: Retrieval (Hybrid Search + Adaptive Reranking)
    t0 = time.time()
    results = searcher.search(search_query, top_k=config.TOP_K_RETRIEVAL)
    search_time = time.time() - t0
    print(f"   üìö {len(results)} chunks retrieved ({search_time:.3f}s)")

    # Show top sources
    for i, (text, score) in enumerate(results[:config.TOP_K_DISPLAY], 1):
        title = text.split("]")[0].lstrip("[") if "[" in text else "‚Äî"
        snippet = text[:80].replace("\n", " ")
        print(f"   [{i}] ({score:.2f}) [{title}] {snippet}...")

    # Stage 3: Generation (Gemini LLM) ‚Äî use ORIGINAL query, not HyDE
    print(f"\n   ü§ñ Generating with {config.GEMINI_MODEL}...")
    t1 = time.time()

    if stream:
        print(f"\n{'‚îÄ' * 60}")
        full_response = ""
        for chunk in generate(query, results[:config.TOP_K_DISPLAY], stream=True):
            print(chunk, end="", flush=True)
            full_response += chunk
        print(f"\n{'‚îÄ' * 60}")
    else:
        full_response = generate(query, results[:config.TOP_K_DISPLAY], stream=False)
        print(f"\n{'‚îÄ' * 60}")
        print(full_response)
        print(f"{'‚îÄ' * 60}")

    gen_time = time.time() - t1
    total = hyde_time + search_time + gen_time

    # Timing summary
    parts = []
    if hyde_time > 0:
        parts.append(f"HyDE: {hyde_time:.2f}s")
    parts.append(f"Search: {search_time:.3f}s")
    parts.append(f"Generate: {gen_time:.2f}s")
    parts.append(f"Total: {total:.2f}s")
    print(f"   ‚è±Ô∏è  {' | '.join(parts)}")

    return full_response


def ask_agentic(query: str, searcher: RAGSearcher, stream: bool = True, use_hyde: bool = True) -> str:
    """
    Agentic RAG pipeline: Decompose ‚Üí Multi-hop Search ‚Üí Evaluate ‚Üí Generate.

    Uses AgenticController to:
      1. Decompose complex queries into sub-queries
      2. Search each sub-query with optional HyDE
      3. Evaluate if information is sufficient
      4. Loop with follow-up queries if needed
      5. Synthesize final answer from all gathered chunks

    Args:
        query: User's question
        searcher: Initialized RAGSearcher instance
        stream: Whether to stream the response
        use_hyde: Whether to use HyDE query transform

    Returns:
        Generated answer string
    """
    t_total = time.time()

    controller = AgenticController(searcher=searcher, use_hyde=use_hyde)

    if stream:
        # Stream mode: print events + stream answer tokens
        full_response = ""

        for event in controller.run_stream_with_answer(query):
            if event.event_type == "token":
                if not full_response:
                    # First token ‚Äî print header
                    print(f"\n{'‚îÄ' * 60}")
                print(event.data["text"], end="", flush=True)
                full_response += event.data["text"]

            elif event.event_type == "done":
                print(f"\n{'‚îÄ' * 60}")
                total_time = time.time() - t_total
                d = event.data
                print(f"   ‚è±Ô∏è  Total: {total_time:.2f}s | "
                      f"Iterations: {d['iterations']} | "
                      f"Chunks: {d['total_chunks']} | "
                      f"Type: {d['query_type']}")

        return full_response

    else:
        # Blocking mode
        result = controller.run(query)

        print(f"\n{'‚îÄ' * 60}")
        print(result.answer)
        print(f"{'‚îÄ' * 60}")

        total_time = time.time() - t_total
        print(f"   ‚è±Ô∏è  Total: {total_time:.2f}s | "
              f"Iterations: {result.iterations} | "
              f"Chunks: {result.total_chunks} | "
              f"Type: {result.query_type}")

        return result.answer


def main():
    """Main entry point with interactive and single-question modes."""
    stream = "--no-stream" not in sys.argv
    use_hyde = "--no-hyde" not in sys.argv
    agentic = "--agentic" in sys.argv
    args = [a for a in sys.argv[1:] if not a.startswith("--")]

    # Determine mode
    mode_name = "Agentic" if agentic else "Classic"
    ask_fn = ask_agentic if agentic else ask

    # Initialize searcher
    print("=" * 60)
    print(f"ü§ñ RAG System ‚Äî {mode_name} Pipeline")
    print(f"   üì° Search: Dense + BM25 + Adaptive Reranking")
    print(f"   ü™Ñ HyDE: {'ON' if use_hyde and config.ENABLE_HYDE else 'OFF'} ({config.GROQ_MODEL})")
    print(f"   üß† LLM:  {config.GEMINI_MODEL}")
    if agentic:
        print(f"   üîÑ Agentic: ON (max {config.AGENTIC_MAX_ITERATIONS} iterations, "
              f"threshold {config.AGENTIC_SUFFICIENCY_THRESHOLD})")
    print("=" * 60)
    searcher = RAGSearcher()
    searcher.load_index()

    if args:
        # Single question mode
        ask_fn(" ".join(args), searcher, stream=stream, use_hyde=use_hyde)
    else:
        # Interactive mode
        print(f"\nüí¨ Interactive mode ({mode_name}) ‚Äî type your question (or 'q' to quit)\n")
        while True:
            try:
                query = input("‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: ").strip()
                if not query or query.lower() in ("q", "quit", "exit"):
                    print("üëã ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö")
                    break
                ask_fn(query, searcher, stream=stream, use_hyde=use_hyde)
                print()
            except KeyboardInterrupt:
                print("\nüëã ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö")
                break


if __name__ == "__main__":
    main()
